
#To run the API run 
#uvicorn app:app
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel
from typing import List, Optional, Dict
from ollama import chat
from ollama import ChatResponse
from rag import perform_search
import pandas as pd

app = FastAPI()

origins = [
    "http://localhost",
    "http://localhost:8000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


AI_MODEL = 'deepseek-r1:8b'
class Prompt(BaseModel):
    messages: List[Dict[str,str]]
    metda_data: Optional[str] = None#Optional string for now

@app.get('get_scenario')
async def generate_scenario():
    '''Picks up a random secnario for setting it up from our prompt database for teacher training. 
    May include future prompts generated by the AI for future cases'''
    df = pd.read_csv('../../scripts/prompts.csv')
    random_scenario = df.sample(1).iloc[0, 0]
    return random_scenario

@app.get('/api/generate')
async def generate(prompts: Prompt):
    '''Generates according to the prompts passed in. It will return plain text to stream it. 
    
    Here is a example of the JSON format that is sent in.
        
        {
    "model": "deepseek-r1",           // Required: Name of the model to use
    "messages": [                // Required: Array of message objects
        {
        "role": "system",        // Role: system, user, or assistant
        "content": "You are a helpful assistant"
        },
        {
        "role": "user",
        "content": "Hello, how are you?"
        },
        {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking!"
        },
        {
        "role": "user",
        "content": "What can you help me with today?"
        }
    ],
    "stream": false,             // Optional: Whether to stream the response
    "options": {                 // Optional: Same options as generate API
        "temperature": 0.7,
        "top_p": 0.9,
        "num_predict": 256
    }
    }
    '''
    
    print(prompts)
    
    async def prompt_gen(prompts):
        results = perform_search(prompts.messages[-1]['content'], 10)
        #We can add more context to more stuff but we will figure that out later. 
        prompts.messages[-1]['content'] = prompts.messages[-1]['content'] + f'\n RESULTS: {results}'
        stream: ChatResponse = chat(model=AI_MODEL, messages=prompts.messages, stream=True)
        
        for chunk in stream:
            yield chunk['message']['content']
    
    return StreamingResponse(prompt_gen(prompts), media_type='text/plain')    
    